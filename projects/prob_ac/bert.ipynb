{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetの定義\n",
    "class BertDataset(Dataset):\n",
    "  def __init__(self, X, y, tokenizer, max_len):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):  # len(Dataset)で返す値を指定\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
    "    text = self.X[index]\n",
    "    inputs = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      truncation=True,\n",
    "      padding=\"max_length\"\n",
    "    )\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "\n",
    "    return {\n",
    "      'ids': torch.LongTensor(ids),\n",
    "      'mask': torch.LongTensor(mask),\n",
    "      'labels': torch.Tensor(self.y[index])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train=pd.read_csv(\"data/train_data.csv\")\n",
    "\n",
    "train,valid = train_test_split(train,test_size=0.2,shuffle=True) \n",
    "train=train.reset_index(drop=True)\n",
    "valid=valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解ラベルのone-hot化\n",
    "y_train = pd.get_dummies(train[\"y\"]).values\n",
    "y_valid = pd.get_dummies(valid[\"y\"]).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Datasetの作成\n",
    "max_len = 20\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset_train = BertDataset(train['title'], y_train, tokenizer, max_len)\n",
    "dataset_valid = BertDataset(valid['title'], y_valid, tokenizer, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: tensor([  101,  4083, 10061,  4275,  2005,  2146,  1011,  9154,  8993,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "labels: tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "for var in dataset_train[0]:\n",
    "  print(f'{var}: {dataset_train[0][var]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "  def __init__(self, drop_rate, otuput_size):\n",
    "    super().__init__()\n",
    "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.drop = torch.nn.Dropout(drop_rate)\n",
    "    self.fc = torch.nn.Linear(768, otuput_size)  # BERTの出力に合わせて768次元を指定\n",
    "    \n",
    "  def forward(self, ids, mask):\n",
    "    _, out = self.bert(ids, attention_mask=mask, return_dict=False)\n",
    "    out = self.fc(self.drop(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, criterion, loader, device):\n",
    "  \"\"\" 損失・正解率を計算\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # デバイスの指定\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # 順伝播\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      # 損失計算\n",
    "      loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # 正解率計算\n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy() # バッチサイズの長さの予測ラベル配列\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # バッチサイズの長さの正解ラベル配列\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "      \n",
    "  return loss / len(loader), correct / total\n",
    "  \n",
    "\n",
    "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
    "  \"\"\"モデルの学習を実行し、損失・正解率のログを返す\"\"\"\n",
    "  # デバイスの指定\n",
    "  model.to(device)\n",
    "\n",
    "  # dataloaderの作成\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "\n",
    "  # 学習\n",
    "  log_train = []\n",
    "  log_valid = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # 開始時刻の記録\n",
    "    s_time = time.time()\n",
    "\n",
    "    # 訓練モードに設定\n",
    "    model.train()\n",
    "    for data in tqdm(dataloader_train):\n",
    "      # デバイスの指定\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # 勾配をゼロで初期化\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "      outputs = model(ids, mask)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "    # 損失と正解率の算出\n",
    "    loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train, device)\n",
    "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid, device)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "    # チェックポイントの保存\n",
    "    #torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
    "\n",
    "    # 終了時刻の記録\n",
    "    e_time = time.time()\n",
    "\n",
    "    # ログを出力\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
    "\n",
    "  return {'train': log_train, 'valid': log_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss_train: 0.6088, accuracy_train: 0.6939, loss_valid: 0.6141, accuracy_valid: 0.6935, 1349.3824sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mセル10 を /Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m cuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# モデルの学習\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m log \u001b[39m=\u001b[39m train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device\u001b[39m=\u001b[39;49mdevice)\n",
      "\u001b[1;32mセル10 を /Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m   outputs \u001b[39m=\u001b[39m model(ids, mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb#X14sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m   loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb#X14sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m   loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb#X14sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hikaru/github/ds-monorepo/projects/prob_ac/bert.ipynb#X14sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# 損失と正解率の算出\u001b[39;00m\n",
      "File \u001b[0;32m~/github/ds-monorepo/.venv/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/github/ds-monorepo/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# パラメータの設定\n",
    "DROP_RATE = 0.4\n",
    "OUTPUT_SIZE = 2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# モデルの定義\n",
    "model = BERTClass(DROP_RATE, OUTPUT_SIZE)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# デバイスの指定\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# モデルの学習\n",
    "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbd86f31783cca70399c6c48c3f5bc4290f66e841790a87d1477b9f5b727528a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
