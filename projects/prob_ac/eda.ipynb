{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_data.csv\")\n",
    "test_df = pd.read_csv(\"data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df[\"keywords\"] = df[\"keywords\"].fillna(\"\").astype(str)\n",
    "    df[\"keywords_count\"] = df[\"keywords\"].str.split(\", \").agg(len)\n",
    "    df.loc[df[\"keywords\"]==\"\", \"keywords_count\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべて小文字にして、ハイフンを半角スペースに変換\n",
    "\n",
    "\n",
    "\n",
    "all_df[\"keywords\"] = all_df[\"keywords\"].str.lower().str.replace(\"-\", \" \")\n",
    "\n",
    "# 複数形の単語リスト\n",
    "plural_words = [\n",
    "    \"models\", \"networks\", \"embeddings\", \"graphs\", \"gans\", \"rnns\", \"parameters\",\n",
    "    \"functions\", \"representations\", \"methods\", \"images\", \"tests\", \"algorithms\", \"names\",\n",
    "    \"records\", \"attributes\", \"coders\", \"recommendations\", \"orders\", \"gradients\", \"tasks\",\n",
    "    \"machines\", \"operations\", \"examples\"\n",
    "]\n",
    "\n",
    "keyword_list = []\n",
    "for idx in all_df.index:\n",
    "    if len(all_df.loc[idx, \"keywords\"]) == 0:\n",
    "        continue\n",
    "    tmp_list = all_df.loc[idx, \"keywords\"].split(\", \")\n",
    "    for plural_word in plural_words:\n",
    "        tmp_list = [word.replace(plural_word, plural_word[:-1]) for word in tmp_list]\n",
    "    keyword_list += tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "from pprint import pprint\n",
    "counter = collections.Counter(keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('deep learning', 1129),\n",
      " ('reinforcement learning', 922),\n",
      " ('representation learning', 420),\n",
      " ('graph neural network', 350),\n",
      " ('neural network', 332),\n",
      " ('generative model', 297),\n",
      " ('meta learning', 283),\n",
      " ('generalization', 240),\n",
      " ('unsupervised learning', 228),\n",
      " ('robustness', 218),\n",
      " ('generative adversarial network', 216),\n",
      " ('gan', 213),\n",
      " ('optimization', 207),\n",
      " ('natural language processing', 206),\n",
      " ('transfer learning', 202),\n",
      " ('self supervised learning', 194),\n",
      " ('deep reinforcement learning', 187),\n",
      " ('interpretability', 184),\n",
      " ('adversarial example', 182),\n",
      " ('computer vision', 176)]\n"
     ]
    }
   ],
   "source": [
    "pprint(counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_len_counter=collections.Counter(all_df[\"keywords_count\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 3165),\n",
      " (4, 2675),\n",
      " (5, 1443),\n",
      " (2, 1311),\n",
      " (0, 1255),\n",
      " (6, 633),\n",
      " (7, 321),\n",
      " (1, 252),\n",
      " (8, 148),\n",
      " (9, 80),\n",
      " (11, 30),\n",
      " (10, 28),\n",
      " (12, 9),\n",
      " (13, 6),\n",
      " (15, 2),\n",
      " (21, 2),\n",
      " (14, 2),\n",
      " (18, 2),\n",
      " (24, 1),\n",
      " (22, 1)]\n"
     ]
    }
   ],
   "source": [
    "pprint(keywords_len_counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lilac.features.nlp.word_vectorizers.w2v_vectorizer import W2VVectorizer\n",
    "from lilac.features.nlp.text_vectorizers.word_vector_based_vectorizer import WordVectorBasedVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23905903,  0.44110057,  0.04697207,  1.0514789 ,  1.3433418 ,\n",
       "       -0.31591403,  0.30191618, -1.2359105 , -1.5443531 ,  0.10688017],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer=W2VVectorizer(10,42,sep=\", \")\n",
    "word_vectorizer.fit(all_df[\"keywords\"])\n",
    "word_vectorizer.transform(\"deep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting required params in WordVectorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.04176531, -0.08907208,  0.01203408, ..., -0.07072501,\n",
       "         0.00845501, -0.15382209],\n",
       "       [ 0.0608058 , -0.12903522,  0.01032884, ..., -0.08803467,\n",
       "         0.01297813, -0.19574483],\n",
       "       [ 0.00524634, -0.00145287, -0.00056168, ..., -0.00181542,\n",
       "         0.00522755,  0.00588716],\n",
       "       ...,\n",
       "       [ 0.02250574, -0.05659229,  0.01666991, ..., -0.04871108,\n",
       "         0.01210373, -0.08806672],\n",
       "       [ 0.03838874, -0.09399089,  0.01279359, ..., -0.06652151,\n",
       "         0.01237313, -0.15337162],\n",
       "       [ 0.0476392 , -0.10085097,  0.01195502, ..., -0.08317403,\n",
       "         0.01206388, -0.16866226]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer=WordVectorBasedVectorizer(\"w2v\",100,how_to_aggregate=\"mean\",sep=\", \")\n",
    "vecs=text_vectorizer.fit_transform(all_df[\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       generative, hierarchical, unsupervised, semisu...\n",
       "1           NLU, word embeddings, representation learning\n",
       "2                                                        \n",
       "3       generative adversarial networks, differential ...\n",
       "4       Generative Models, Latent representations, Pre...\n",
       "                              ...                        \n",
       "4969    Neural Processes, Deep Sets, Translation Equiv...\n",
       "4970                margin, homogeneous, gradient descent\n",
       "4971    adversarial examples, adversarial training, pr...\n",
       "4972    Question Answering, Multi-Hop QA, Deep Learnin...\n",
       "4973                                   federated learning\n",
       "Name: keywords, Length: 4974, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"keywords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lilac-ngcUsNlu-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8de8c3fc8992900265f19d9bb0fcb1899f266562d481daf9b8377301424b898"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
